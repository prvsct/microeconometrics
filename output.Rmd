---
title: "Microeconometrics Report I: *Does Working from Home Work? Evidence from a Chinese Experiment¹*"
author: "Pedro Scatimburgo"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: readable
    toc: true
    number_sections: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(tidyverse)
```

<style type="text/css">

h1.title {
  font-size: 24px;
  color: black;
  text-align: center;
}
subtitle {
  font-size: 24px;
  color: black;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 16px;
  font-family: "Kaleko205Text-Bold", Kaleko 205, Bold;
  color: black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 16px;
  font-family: "Kaleko205Text-Bold", Kaleko 205, Bold;
  color: black;
  text-align: center;
}
</style>

> ¹Nicholas Bloom, James Liang, John Roberts, Zhichun Jenny Ying, **Does Working from Home Work? Evidence from a Chinese Experiment**, The Quarterly Journal of Economics, Volume 130, Issue 1, February 2015, Pages 165–218. *[Link to the lastest version.](https://drive.google.com/file/d/1DPhkrgydBA7Xt9ZHHQv8ZcpBSbzgfy-F/)*. *[Link to data.](https://drive.google.com/file/d/1xvEVI74v3xsfnwNvRwyOz9Mup-aNYiE8/view?usp=sharing/)*


# Briefly summarize the research question in the paper, and why it is interesting. What is the main parameter of interest?

The authors try to understand the impact of working from home (WFH) on firms and workers. Although there has always been interest in WFH practices, it wasn't until the COVID-19 Pandemic that WFH was widely adopted worldwide. This particular paper is interesting because its experiment was conducted between 2012 and 2013, much before the pandemic and the dissemination of WFH after 2020.

Even before the pandemic, WFH practices sparked great interest because they presented a myriad of possibilities and worries for both firms and workers: for firms, it could reduce costs, but at the same time reduce productivity because of lack of supervision and miscommunication; for workers, it could improve well-being by allowing more time with family -- especially in the case of parents --, but it could create a work-life misbalance. Furthermore, WFH also has public policy implications related to commuting and the use of urban spaces.

There are several parameters of interest related to both firms and workers. For the firms, the main parameter of interest is the overall employee performance, a normalized z-score measure. Other parameters include phone calls and minutes on the phone. For the workers, the main parameter is satisfaction. Other parameters include attrition and exhaustion.

# Briefly describe the design of the experiment.

The experiment consisted in randomly assigning call center workers from a large Chinese travel agency to work from home for four days a week, while attending the office for one day a week. The workers were organized in teams consisting of 10 to 15 employees who operated on the same shifts, which was determined in advance by the firm. Randomization was performed at the individual level: on the same team, there were workers from the treatment (WFH) and comparison (office-based) groups. Individuals from the same team worked on the same schedules and had the same supervisor (who was always office-based), besides receiving an identical number of incoming calls. All workers used the same software and equipment provided by the firm, faced the same pay structure, and received the same training. For the treatment group, however, training was received only on the day they were in the office. An entire team could have their shifts changed, but individual workers from the both the treatment and the comparison groups were not allowed to work overtime. Thus, the authors argue that treatment changed only the location of work.

The employees were previously informed of the WFH program. They all took a survey that assessed their qualification and willingness to participate, without being informed of the participation criteria. Among the workers deemed qualified for the program, 503 (51%) volunteered. The ualifications for the program required that workers had a tenure of at least six months, broadband access to the Internet, and an independent workspace. Among the volunteers, 249 (50%) met the requirements and were recruited. This way, one could argue that the experiment had an oversubscription method: with limited implementation capacity, demand for the WFH program exceeded supply. Since the design of the experiment was selected from a sample that was already interested in the program to begin with, there is selection bias. Indeed, the authors report that those who volunteered for the program tended to have longer commuting time, less tenure in the firm, and be less educated. A public lottery based on birthdates then assigned workers for the treatment and control groups.

# What is the minimum detectable effect for the main parameter of interest?  Did the study have adequate power? (you can use the authors’ data to answer this question)

Since the authors focus both on the firm and on the workers, I have conducted separated analysis for the two main parameters of interest: overall performance and satisfaction.

First, I will load the files uploaded in the Nicholas Bloom's personal page. It will import 22 files, one for each of the analysis conducted by the authors. I will use only two dataframes: ``wfh/data/performance_during_exper.dta`` and ``wfh/data/satisfaction.dta``, which I will save on objects ``perormance`` and ``satisfaction``. These dataframes contain values for several variables, including overall performance, ``perform1``, and satisfaction, ``satisfaction``. Below I print the most relevant variables from each dataframe.

```{r}
files <- fs::dir_ls("wfh/data") %>%
  purrr::map(haven::read_dta)

performance <- files[["wfh/data/performance_during_exper.dta"]]
satisfaction <- files[["wfh/data/satisfaction.dta"]]
```

```{r}
knitr::kable(head(performance %>%
                    select(
                      personid,
                      year_week,
                      perform1,
                      expgroup,
                      treatment,
                      experiment_treatment
                      )
                  )
             )
```

```{r}
knitr::kable(head(satisfaction %>%
                    select(
                      personid,
                      satisfaction,
                      expgroup_treatment,
                      )
                  )
             )
```

## Some preliminary data analysis

First let's see if the data makes sense. The experiment was conducted for 85 weeks. There are 249 observations: 131 in the treatment group and 118 in the control group. The ``performance`` dataframe includes not only the individuals that participated in the experiment, but also individuals from other branches (which the authors use for some of the analysis). The ``satisfaction`` dataframe should also contain 249 individuals, who were surveyed for five times. So let's filter the ``performance`` dataframe and check these numbers:

```{r}
performance %>%
  dplyr::filter((!year_week %in% c(201049) & expgroup %in% c(0,1))) %>%
  summarise(
    total_individuals = length(unique(personid)),
    weeks = length(unique(year_week))
  )

satisfaction %>%
  summarise(
    individuals = length(unique(personid)),
    surveys = length(unique(surveyno))
  )
```

The ``performance`` dataframe contains 249 individuals and 85 weeks, as expected. The ``satisfaction`` dataframe contains 5 surveys, but only 171 individuals. I could not figure out why the number of individuals is not 249. The authors don't mention anything about this in the paper or in the online appendix. Still, 855 is the number of observations reported by the authors in the satisfaction regression, and as we will see, we can replicate the authors' results exactly.

## Overall performance (firm's parameter)

First let's see if we can replicate the authors' results (or at least get close to them). We want to run the following regression:

\begin{equation}
Employee Performance_{i,t} = \alpha Treat_i \times Experiment_t + \beta_t + \gamma_i + \epsilon_{i,t}
\end{equation}

This is the Stata code I am trying to replicate:

```
*col(1)
xi:areg perform1 experiment_treatment i.year_week if year_week~=201049 &(expgroup==1|expgroup==0),ab(personid) cluster(personid)
```

From my understanding, this code is estimating a regression of ``perform1`` against ``experiment_treatment``, removing the ``year_week`` of ``201049`` from the sample, selecting only values for ``exproup`` of ``1`` or ``0``, clustering in ``personid`` and with fixed effects of ``personid`` and ``year_week``. That is, $Employee Performance_{i,t}$ is ``perform1`` and $Treat_i \times Experiment_t$ is ``experiment_treatment``. In R, using ``estimatr::lm_robust``, this would be something like (I use ``se_type = "stata"`` to get as close as possible to the authors' results; [click here for a discussion on the possible values for this argument](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html "Who still uses Stata though?")):

```{r}

reg1 <- performance %>%
  dplyr::filter((!year_week %in% c(201049) & expgroup %in% c(0,1))) %>%
  estimatr::lm_robust(formula = perform1 ~ experiment_treatment,
                      clusters = personid,
                      fixed_effects = ~personid + year_week,
                      se_type = "stata")

summary(reg1)
```

Which give us a estimate of ``r reg1$coefficients[1]``, a standard error of ``r reg1$std.error[1]``. The authors report values of 0.232 and 0.063, respectively. We did it!

To find the minimum detectable effect for performance I will use the following formula:

\begin{equation}
\bar{\beta} = (t_{\frac{\alpha}{2}}+t_{1-\kappa})\sigma_{\hat{\beta}}
\end{equation}

Where $\bar{\beta}$ is the minimum detectable effect (MDE) given a statistical significance level of $\alpha$ and a power of $\kappa$. Following what we did in class, I will use a significance level of 95% and a power of 80% in a bicaudal test, that is: ``alpha = 0.975`` and ``kappa = 0.80``. $\sigma_{\hat{\beta}}$ is the standard deviation of the estimator. We can use its estimator, $\hat{\sigma}_{\hat{\beta}}$, which ``lm_robust`` already gave to us. Then, assuming normality, the minimum detectable effect for this parameter is:

```{r}
alpha <- 0.975
kappa <- 0.80

t_alpha <- qnorm(p = alpha)
t_kappa_c <- qnorm(p = 1-kappa, lower.tail = F)

(mde1 = (t_alpha + t_kappa_c)*reg1$std.error[1])
```
So the minimum detectable effect for the performance parameter is ``r mde1``. Recall that this variable is normalized, so the minimum detectable effect is approximately ``r round(mde1,3)`` standard deviation. This seems like a very reasonable value for a MDE, specially considering that the estimated effect is approximately ``r round(reg1$coefficients[1],3)`` standard deviation.

Now let's plug the estimated effect into this formula to find the power of the test, for a level of significance of 95% and the same standard error. We find that:

```{r}
t_kappa_c_plugged <- reg1$coefficients[1]/reg1$std.error[1] - t_alpha

pnorm(q = t_kappa_c_plugged)
```
So the test had a power of approximately ``r paste0(round(pnorm(q = t_kappa_c_plugged),3)*100,"%")``, which is pretty high. So the test had adequate power.

## Satisfaction (workers' parameter)

Here I will be briefer. I want to replicate the following code:

```
use satisfaction.dta, clear
foreach i in satisfaction general life {
gen ln`i'=ln(`i')
xi:areg ln`i' expgroup_treatment i.surveyno, ab(personid) cluster(personid)
}
```

From my understanding, this code estimates a regression of ``log(satisfaction)`` against ``expgroup_treatment``, clustered at ``personid`` and with fixed effects of ``surveyno`` and ``personid``. Then:


```{r}

reg2 <- satisfaction %>%
  estimatr::lm_robust(formula = log(satisfaction) ~ expgroup_treatment,
  clusters = personid,
  fixed_effects = ~personid + surveyno,
  se_type = "stata")

summary(reg2)
```

Again we are able to replicate the results. Since the code is analogous, I will simply paste it and make the appropriate changes:

```{r}
alpha <- 0.975
kappa <- 0.80

t_alpha <- qnorm(p = alpha)
t_kappa_c <- qnorm(p = 1-kappa, lower.tail=F)

(mde2 = (t_alpha + t_kappa_c)*reg2$std.error[1])
```

So the minimum detectable effect for the performance parameter is ``r mde2``. This time, the estimated effect is just under the MDE. This means that the desired power or the significance level are too high. However, we know it is not the significance level, since the p-value is ``r round(reg2$p.value[1],3)``, which means we reject the null hypothesis at the 5% level for bicaudal test. So the test must have a power smaller than 80%. Indeed, plugging the estimated effect in the MDE formula, we find that:

```{r}
t_kappa_c_plugged <- reg2$coefficients[1]/reg2$std.error[1] - t_alpha

pnorm(q = t_kappa_c_plugged)


```

This time the power of the test is of approximately ``r paste0(round(pnorm(q = t_kappa_c_plugged),3)*100,"%")``. Even though 80% is an arbitrary level, it's a relatively common one, so I am tempted to say that the power is just a bit underpowered.

# Calculate randomization inference p-values, using as the estimator $\hat{\beta}$ and a t-statistict $t = \frac{\hat{\beta}}{se(\hat{\beta})}$.  In general, explain the reasons why those p-values may differ between each other, and also why they can differ from p-values based on the asymptotic distribution of the estimator. How do these p-values differ in your example?

**Remark 1:** since randomization inference is very hardware-demanding, I decided to conduct the permutations only in the overall performance parameter. I chose performance instead of satisfaction because it has a larger sample size and because it contains all individuals. Also, unlike the ``performance`` dataframe which allowed me to create a ``experiment`` variable using ``year_week``, this was not possible with the ``satisfaction`` parameter (more on that below).

Our estimator and t-statistic are, respectively:

```{r}
(reg1_estimator <- reg1$coefficients[1])

(reg1_tstat <- reg1$coefficients[1]/reg1$std.error[1])
```

To calculate randomization inference p-values, we treat our potential outcomes as non-stochastic and assume a sharp null: $H_0: Y_i(1) = Y_i(0), \forall i$. Under this sharp null, we are able to derive the exact distribution of $t(T,Y)$. For all possible assignments $\tilde{T}$, we calculate $t(\tilde{T},Y)$ and check the proportion of $t(\tilde{T},Y)$ that is greater (in absolute value) than $t(\tilde{T},Y)$.

However, for this to work, we need to know the random allocation mechanism. The randomization was performed accordingly to the worker's birthdates: workers with even birthdates were assigned to work from home. This way, I will assume that the allocation mechanism follows a binomial distribution with parameter equal to the ratio between treated and control individuals.

In practice, I will permutate the values of ``performance$treatment`` using ``rbinom`` and save the new values in ``tilde_T``, then estimate a new regression for each permutation, and save each new t-statistic. I will perform ``P = 10^4`` permutations. Note that since the independent variable is actually ``experiment_treatment`` instead of ``treatment``, I will first create a ``experiment`` variable, which takes value ``1`` only after the treatment started, that is, if ``year_week >= 201050``, and then create the ``experiment_treatment_T`` variable, that will be the independent variable in the regression. I do this in the ugly ``for`` loop below, which will possibly burn my laptop's CPU irredeemably:

```{r eval=FALSE, include=TRUE}
P <- 10^4

# Creates a vector that will receive each t-statistic and each beta. It's an empty vector, which means it doesnt have a size yet. Therefore it can receive any number of elements.
tstat_ri <- c()
beta_ri <- c()

# Still out of the loop, I filter performance and select only the most relevant variables to make the loop as bit faster. I wont change a thing though. I also create the experiment variable (surprisingly it didnt existed in the original dataframe)
performance_ri <- performance %>%
  dplyr::filter((!year_week %in% c(201049) & expgroup %in% c(0,1))) %>%
  select(personid, perform1, treatment, year_week) %>%
  mutate(
    experiment = ifelse(year_week >= 201050, 1, 0)
  )

# Now for the proper loop. It runs through every value between 1 and P. Each value of i = {1,2,...,1000} is the i-th step of the iteration but also the i-th position of the tstat_ri list that will receive each of the t-statistics
for(i in 1:P){
  
# I do this to have some idea of how long this is gonna take. It prints every i that is divisible by 100 and the time when the printing occurred. Printing all i would require a lot of memory
  if(i %% 100 == 0){
    print(paste0(i,", ",Sys.time()))
  }
  
# Here I do the actual permutations using rbinom and create a new tilde_T variable. In each iteration, rbinom() runs again and creates a whole new vector of zeros and ones that will be stored in tilde_T, which is the column that takes the values of the treatment T_i. Then I create experiment_treatment_T. Also, note that the same person i must have the same value of tilde_T for every value of year_week, that is, if the individual is treated in the first week, they she must be treated in all weeks, and vice-versa. To do this, I first group the dataframe by personid and then use n=1 in the rbinom
performance_ri <- performance_ri %>%
  group_by(personid) %>%
  dplyr::mutate(
    tilde_T = rbinom(n = 1, size = 1, prob = 131/249),
    experiment_treatment_T = experiment*tilde_T
  ) %>%
  ungroup()

# Now I estimate the regression.
reg1_ri <- performance_ri %>%
  estimatr::lm_robust(formula = perform1 ~ experiment_treatment_T,
                      clusters = personid,
                      fixed_effects = ~personid + year_week,
                      se_type = "stata")

# And save each t-statistic. I will simply take the t-value instead of calculating the statistic by hand to make things a bit faster. In each iteration i, I will store the t-statistic and the estimated beta in the i-th position of their respective vectors
tstat_ri[i] <- reg1_ri$statistic[1]
beta_ri[i] <- reg1_ri$coefficients[1]
  
}

# Remember we need to take absolute values
tstat_ri <- abs(tstat_ri)
beta_ri <- abs(beta_ri)
```


```{r include=FALSE}
load("tstat_ri_10.Rdata")
load("beta_ri_10.Rdata")
```


With all $t(\tilde{T},Y)$ stored in ``tstat_ri`` and ``beta_ri``, we can calculate the proportion of these statistics that are larger than ``r reg1_tstat`` and ``r reg1_estimator``, respectively:

```{r}
sum(tstat_ri > reg1_tstat)/10^4
sum(beta_ri > reg1_estimator)/10^4
```

The p-values obtained through permutation tests of the t-statistic $t = \frac{\hat{\beta}}{se(\hat{\beta})}$ and the estimator $\hat{\beta}$ are identical and equal to ``sum(tstat_ri > reg1_tstat)/P``.

```{r include=FALSE}
load("tstat_ri.Rdata")
```

The graphs below plot the density of the values obtained through the permutation tests and the value observed in the actual sample. Please note these are absolute values.

```{r}

ggplot() + 
  geom_density(mapping = aes(x = tstat_ri), fill = "grey") + 
  geom_vline(aes(xintercept = reg1$statistic[1]), color = "red") +
  geom_text(aes(x = reg1$statistic[1]+0.1), label = "Observed value", y = 0.15, angle = 90, vjust = -0.2) +
  labs(title = "Density of simulated t-statistics and observed value",x = "Values", y = "Density") +
  theme_bw(base_size = 10)

ggplot() + 
  geom_density(mapping = aes(x = beta_ri), fill = "grey") + 
  geom_vline(aes(xintercept = reg1$coefficients[1]), color = "red") +
  geom_text(aes(x = reg1$coefficients[1]+0.006), label = "Observed value", y = 2, angle = 90, vjust = -0.2) +
  labs(title = "Density of simulated estimators and observed value", x = "Values", y = "Density") +
  theme_bw(base_size = 10)
```


**Remark 3:** I also executed a for loop with ``P = 10^5`` permutations (the code took 4 hours to run), but only for the t-statistic (I got the question wrong, which I only realized after the aforementioned 4 hours). The calculated p-value was ``r as.double(sum(tstat_ri > reg1_tstat)/10^5)``. Note that the original p-value is ``r as.double(reg1$p.value[1]/10^5)``. These values are nearly identical, but I don't know what to make of this.

# Discuss potential problems in the experiment.  Discuss how you would be able to check whether these problems are relevant (for example, by looking at the available data or by collecting more data), or how the experiment could have been done differently to avoid such problems. Be clear about whether the potential problems you raised were discussed in the paper.

## Implications of the oversubscription method

## Imperfect compliance

## Hawthorne and Gift-Exchange Effects

## Attrition

## Negative difference in performance compared to baseline

