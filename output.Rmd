---
title: "Microeconometrics Report I: *Does Working from Home Work? Evidence from a Chinese Experiment¹*"
author: "Pedro Scatimburgo"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: readable
    toc: true
    number_sections: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(tidyverse)
```

<style type="text/css">

h1.title {
  font-size: 24px;
  color: black;
  text-align: center;
}
subtitle {
  font-size: 24px;
  color: black;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 16px;
  font-family: "Kaleko205Text-Bold", Kaleko 205, Bold;
  color: black;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 16px;
  font-family: "Kaleko205Text-Bold", Kaleko 205, Bold;
  color: black;
  text-align: center;
}
</style>

> ¹Nicholas Bloom, James Liang, John Roberts, Zhichun Jenny Ying, **Does Working from Home Work? Evidence from a Chinese Experiment**, The Quarterly Journal of Economics, Volume 130, Issue 1, February 2015, Pages 165–218. *[Link to the lastest version.](https://drive.google.com/file/d/1DPhkrgydBA7Xt9ZHHQv8ZcpBSbzgfy-F/)*. *[Link to data.](https://drive.google.com/file/d/1xvEVI74v3xsfnwNvRwyOz9Mup-aNYiE8/view?usp=sharing/)*

To run this code you will have to install and load ``tidyverse``, ``haven`` and ``estimatr``. ``tidyverse`` is the core of all the data wrangling. ``haven`` is used only for importing ``.dta`` files. ``estimatr`` performs robust estimations, including implementations for clusters and fixed effects. You also have to download the data from the link above and save the ``wfh`` folder in your working directory.

```{r eval=FALSE, include=TRUE}
install.packages("tidyverse")
install.packages("haven")
install.packages("estimatr")

library(tidyverse)
library(haven)
library(estimatr)
```

# Briefly summarize the research question in the paper, and why it is interesting. What is the main parameter of interest?

The authors try to understand the impact of working from home (WFH) on firms and workers. Although there has always been interest in WFH practices, it wasn't until the COVID-19 Pandemic that WFH was widely adopted worldwide. This particular paper is interesting because its experiment was conducted between 2012 and 2013, much before the pandemic and the dissemination of WFH after 2020.

Even before the pandemic, WFH practices sparked great interest because they presented a myriad of possibilities and worries for both firms and workers: for firms, it could reduce costs, but at the same time reduce productivity because of lack of supervision and miscommunication; for workers, it could improve well-being by allowing more time with family -- especially in the case of parents --, but it could create a work-life misbalance. Furthermore, WFH also has public policy implications related to commuting and the use of urban spaces.

There are several parameters of interest related to both firms and workers. For the firms, the main parameter of interest is the overall employee performance, a normalized z-score measure. Other parameters include phone calls and minutes on the phone. For the workers, the main parameter is satisfaction. Other parameters include attrition and exhaustion.

# Briefly describe the design of the experiment.

The experiment consisted in randomly assigning call center workers from a large Chinese travel agency to work from home for four days a week, while attending the office for one day a week. The workers were organized in teams consisting of 10 to 15 employees who operated on the same shifts, which was determined in advance by the firm. Randomization was performed at the individual level: on the same team, there were workers from the treatment (WFH) and comparison (office-based) groups. Individuals from the same team worked on the same schedules and had the same supervisor (who was always office-based), besides receiving an identical number of incoming calls. All workers used the same software and equipment provided by the firm, faced the same pay structure, and received the same training. For the treatment group, however, training was received only on the day they were in the office. An entire team could have their shifts changed, but individual workers from the both the treatment and the comparison groups were not allowed to work overtime. Thus, the authors argue that treatment changed only the location of work.

The employees were previously informed of the WFH program. They all took a survey that assessed their qualification and willingness to participate, without being informed of the participation criteria. Among the workers deemed qualified for the program, 503 (51%) volunteered. The qualifications for the program required that workers had a tenure of at least six months, broadband access to the Internet, and an independent workspace. Among the volunteers, 249 (50%) met the requirements and were recruited. A public lottery based on birthdates then assigned workers for the treatment and control groups. The experiment was then conducted for 85 weeks. For the duration of the experiment, the workers had several measures of performance tracked. The firm also conducted 5 surveys to measure the workers' satisfaction with their jobs. The research design involved problems of selection bias, attrition and imperfect compliance which will be discussed in question 5.

# What is the minimum detectable effect for the main parameter of interest?  Did the study have adequate power? (you can use the authors’ data to answer this question)

Since the authors focus both on the firm and on the workers, I have conducted separated analysis for the two main parameters of interest: overall performance and satisfaction.

## Some preliminary data analysis

First, I will load the files uploaded to Nicholas Bloom's page. It will import 22 files, one for each of the analyses conducted by the authors. I will use only two dataframes: ``wfh/data/performance_during_exper.dta`` and ``wfh/data/satisfaction.dta``, which I will save on objects ``performance`` and ``satisfaction``. These dataframes contain values for several variables, including overall performance, ``perform1``, and satisfaction, ``satisfaction``. I will select only the variables relevant to the estimations I will conduct here. Also, as we will see below, we have to filter some individuals from the ``performance`` dataframe to include only the 249 individuals who were part of the experiment, since it also contains data about individuals in other firms that were used for robustness tests.

```{r}
files <- fs::dir_ls("wfh/data") %>%
  purrr::map(haven::read_dta) # Reads all files in the wfh/data folder and import them all into the files object. purrr::map creates a list

# We are interested only in the performance and satisfaction dataframes. I keep the other dataframes in case I might need them
performance <- files[["wfh/data/performance_during_exper.dta"]]
satisfaction <- files[["wfh/data/satisfaction.dta"]]
```

```{r}
performance <- performance %>%
  dplyr::filter((!year_week %in% c(201049) & expgroup %in% c(0,1))) %>% # Filters the dataframe to include only the data the authors use for this estimation
  dplyr::select(personid, year_week, perform1, treatment, experiment_treatment) # Select only the variables we will use

knitr::kable(head(performance)) # Plots the first rows of the dataframe as example.
```

```{r}
satisfaction <- satisfaction %>%
  select(personid, surveyno, satisfaction, expgroup_treatment)

knitr::kable(head(satisfaction))
```

Now let's see if the data makes sense. The experiment was conducted for 85 weeks. There are 249 observations: 131 in the treatment group and 118 in the control group. The ``performance`` dataframe includes not only the individuals that participated in the experiment but also individuals from other branches (which the authors use for some of the analysis). The ``satisfaction`` dataframe should also contain 249 individuals, who were surveyed five times. So let's filter the ``performance`` dataframe and check these numbers:

```{r}
performance %>%
  dplyr::summarise( # summarise is a function that summarises data for a lack of better way to explain
    total_individuals = length(unique(personid)), # How many unique individuals are there in the dataframe?
    weeks = length(unique(year_week)) # How many unique weeks are there in the dataframe?
  )

satisfaction %>%
  dplyr::summarise(
    individuals = length(unique(personid)),
    surveys = length(unique(surveyno))
  )
```

The ``performance`` dataframe contains 249 individuals and 85 weeks, as expected. The ``satisfaction`` dataframe contains 5 surveys, but only 171 individuals. I could not figure out why the number of individuals is not 249. The authors don't mention anything about this in the paper or the online appendix. Still, 855 is the number of observations reported by the authors in the satisfaction regression, and as we will see, we can replicate the authors' results exactly.

## Overall performance (firm's parameter)

Let's replicate the authors' results (or at least get close to them). We want to run the following regression:

\begin{equation}
Employee Performance_{i,t} = \alpha Treat_i \times Experiment_t + \beta_t + \gamma_i + \epsilon_{i,t}
\end{equation}

This is the Stata code I am trying to replicate:

```
*col(1)
xi:areg perform1 experiment_treatment i.year_week if year_week~=201049 &(expgroup==1|expgroup==0),ab(personid) cluster(personid)
```

From my understanding, this code is estimating a regression of ``perform1`` against ``experiment_treatment``, removing the ``year_week`` of ``201049`` from the sample, selecting only values for ``exproup`` of ``1`` or ``0``, clustering in ``personid`` and with fixed effects of ``personid`` and ``year_week``. That is, $Employee Performance_{i,t}$ is ``perform1`` and $Treat_i \times Experiment_t$ is ``experiment_treatment``. I save the results of the regression in the ``reg1`` object. In R, using ``estimatr::lm_robust``, this would be something like (I use ``se_type = "stata"`` to get as close as possible to the authors' results; [click here for a discussion on the possible values for this argument](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html "Who still uses Stata though?")):

```{r}

reg1 <- performance %>%
  estimatr::lm_robust(formula = perform1 ~ experiment_treatment, # perform1 is regressed against experiment_treatment
                      clusters = personid, # Clusters in personid, which is the individual
                      fixed_effects = ~personid + year_week, # Fixed effects for both the individual and the week
                      se_type = "stata") # We use Stata standard errors

summary(reg1) # Prints the results
```

This gives us an estimate of ``r reg1$coefficients[1]``, a standard error of ``r reg1$std.error[1]``. The authors report values of 0.232 and 0.063, respectively. We did it!

To find the minimum detectable effect for performance I will use the following formula:

\begin{equation}
\bar{\beta} = \left(t_{\frac{\alpha}{2}}+t_{1-\kappa}\right)\sigma_{\hat{\beta}}
\end{equation}

Where $\bar{\beta}$ is the minimum detectable effect (MDE) given a statistical significance level of $\alpha$ and power of $\kappa$. Following what we did in class, I will use a significance level of 95% and a power of 80% in a bicaudal test, that is: ``alpha_2 = 0.975`` and ``kappa = 0.80``. $\sigma_{\hat{\beta}}$ is the standard deviation of the estimator. We can use its estimator, $\hat{\sigma}_{\hat{\beta}}$, which ``lm_robust`` already gave to us as the standard error of the estimator in the object ``reg1$std.error[1]``. Then, assuming normality, the minimum detectable effect for this parameter is:

```{r}
alpha_2 <- 0.975 # 2,5% for each side (bicaudal)
kappa <- 0.80

t_alpha_2 <- qnorm(p = alpha_2) # Returns the critical value in a normal distribution for a probability of alpha_2
t_kappa_c <- qnorm(p = kappa) # We use kappa instead of 1-kappa because by default qnorm uses P[X <= x], if we used 1-kappa we would have to set lower.tail = F

(mde1 = (t_alpha_2 + t_kappa_c)*reg1$std.error[1])
```

So the minimum detectable effect for the performance parameter is ``r mde1``. Recall that this variable is normalized, so the minimum detectable effect is approximately ``r round(mde1,3)`` standard deviation. This seems like a very reasonable value for an MDE, especially considering that the estimated effect is approximately ``r round(reg1$coefficients[1],3)`` standard deviation.

Now we plug the estimated effect into this formula to find the power of the test $(\bar{\beta} = \hat{\beta})$, for a level of significance of 95% and the same standard error:

\begin{equation}
\bar{\beta} = \left(t_{\frac{\alpha}{2}}+t_{1-\kappa}\right)\sigma_{\hat{\beta}} \implies t_{1-\kappa} = \frac{\hat{\beta}}{\hat{\sigma}_{\hat{\beta}}} - t_{\frac{\alpha}{2}}
\end{equation}

We find that:

```{r}
t_kappa_c_plugged <- reg1$coefficients[1]/reg1$std.error[1] - t_alpha_2

pnorm(q = t_kappa_c_plugged)
```

So the test had a power of approximately ``r paste0(round(pnorm(q = t_kappa_c_plugged),3)*100,"%")``, which is pretty high. So the test had adequate power.

## Satisfaction (workers' parameter)

Here I will be briefer. I want to replicate the following code:

```
use satisfaction.dta, clear
foreach i in satisfaction general life {
gen ln`i'=ln(`i')
xi:areg ln`i' expgroup_treatment i.surveyno, ab(personid) cluster(personid)
}
```

From my understanding, this code estimates a regression of ``log(satisfaction)`` against ``expgroup_treatment``, clustered at ``personid`` and with fixed effects of ``surveyno`` and ``personid``. Then:


```{r}

reg2 <- satisfaction %>%
  estimatr::lm_robust(formula = log(satisfaction) ~ expgroup_treatment,
  clusters = personid,
  fixed_effects = ~personid + surveyno,
  se_type = "stata")

summary(reg2)
```

Again we are able to replicate the results. Since the code is analogous, I will simply paste it and make the appropriate changes:

```{r}
alpha_2 <- 0.975
kappa <- 0.80

t_alpha_2 <- qnorm(p = alpha_2)
t_kappa_c <- qnorm(p = kappa)

(mde2 = (t_alpha_2 + t_kappa_c)*reg2$std.error[1])
```

So the minimum detectable effect for the performance parameter is ``r mde2``. This time, the estimated effect is just under the MDE. This means that either the desired power or the significance level is too high. However, we know it is not the significance level, since the p-value is ``r round(reg2$p.value[1],3)``, which means we reject the null hypothesis at the 5% level for the bicaudal test. So the test must have a power smaller than 80%. Indeed, plugging the estimated effect in the MDE formula, we find that:

```{r}
t_kappa_c_plugged <- reg2$coefficients[1]/reg2$std.error[1] - t_alpha_2

pnorm(q = t_kappa_c_plugged)


```

This time the power of the test is of approximately ``r paste0(round(pnorm(q = t_kappa_c_plugged),3)*100,"%")``. Even though 80% is an arbitrary level, it's a relatively common one, so I am tempted to say that the power is just a bit underpowered.

# Calculate randomization inference p-values, using as the estimator $\hat{\beta}$ and a t-statistict $t = \frac{\hat{\beta}}{se(\hat{\beta})}$.  In general, explain the reasons why those p-values may differ between each other, and also why they can differ from p-values based on the asymptotic distribution of the estimator. How do these p-values differ in your example?

**Remark 1:** since randomization inference is very hardware-demanding, I decided to conduct the permutations only in the overall performance parameter. I chose performance instead of satisfaction because it has a larger sample size and because it contains all individuals. Also, unlike the ``performance`` dataframe which allowed me to create a ``experiment`` variable using ``year_week``, this was not possible with the ``satisfaction`` parameter (more on that below).

Our estimator and t-statistic are, respectively:

```{r}
(reg1_estimator <- reg1$coefficients[1])

(reg1_tstat <- reg1$coefficients[1]/reg1$std.error[1])
```

To calculate randomization inference p-values, we treat our potential outcomes as non-stochastic and assume a sharp null: $H_0: Y_i(1) = Y_i(0), \forall i$. Under this sharp null, we can derive the exact distribution of $t(T,Y)$. For all possible assignments $\tilde{T}$, we calculate $t(\tilde{T},Y)$ and check the proportion of $t(\tilde{T},Y)$ that is greater (in absolute value) than $t(\tilde{T},Y)$.

However, for this to work, we need to know the random allocation mechanism. The randomization was performed accordingly to the worker's birth dates: workers with even birth dates were assigned to work from home. This way, I will assume that the allocation mechanism follows a binomial distribution with a parameter equal to the ratio between treated and control individuals, which is ``131/249 = 0.526``, which is very close to ``0.5``.

In practice, I will permutate the values of ``performance$treatment`` using ``rbinom`` and save the new values in ``tilde_T``, then estimate a new regression for each permutation, and save each new t-statistic. I will perform ``P = 10^4`` permutations. Note that since the independent variable is actually ``experiment_treatment`` instead of ``treatment``, I will first create a ``experiment`` variable, which takes value ``1`` only after the beginning of the treatment, that is, if ``year_week >= 201050``, and then create the ``experiment_treatment_T`` variable, that will be the independent variable in the regression. This is only necessary because of how the ``performance`` dataframe is constructed: if it didn't include values of outcomes from before the treatment started, we could simply use ``tilde_T`` as a regressor.

Also, we have to consider that an individual who, in a certain allocation $\tilde{T}$, receives in the week $t = 1$, a treatment status $T_i = 1$, should have a treatment status of $T_i=1$ for all subsequent weeks. That is, we cannot simply randomize the whole ``performance$treatment`` column without considering the clusters of individuals. For each individual $i$, we should have (for individuals who have not suffered attrition) a total of 85 rows. All of these rows should receive the same randomized value of ``treatment`` (and therefore of ``experiment_treatment_T``). The best solution is simply to group the dataframe per individual, using ``dplyr::group_by(personid)``. Since the grouping makes the dataframe behave in such a way that each cluster is a separate dataframe, and because ``rbinom`` is a vectorized function, we use as argument ``n = 1``. If we had no clusters, then we wouldn't group the dataframe, and the number of observations ``n`` would be the number of individuals.

I do this in the ugly ``for`` loop below, which will possibly burn my laptop's CPU irredeemably:

```{r eval=FALSE, include=TRUE}
P <- 10^4

# Creates the vectors that will receive each t-statistic and each beta. They are empty vectors, which means they dont have a size yet. Therefore they can receive any number of elements.
tstat_ri <- c()
beta_ri <- c()

# Still out of the loop, I create the experiment variable (surprisingly it didnt existed in the original dataframe). This step wouldnt be necessary if the data didnt include values from before the treeatment started. I didn't have to do this in the first regression saved in the reg1 object because I used the experiment_treatment variable.
performance_ri <- performance %>%
  dplyr::mutate(
    experiment = ifelse(year_week >= 201050, 1, 0)
  )

# Now for the proper loop. It runs through every value between 1 and P. Each value of i = {1,2,...,10000} is the i-th step of the iteration but also the i-th position of the tstat_ri list that will receive each of the t-statistics

for(i in 1:P){# Loop starts here
  
# I do this to have some idea of how long this is gonna take. It prints every i that is divisible by 100 and the time when the printing occurred. Printing all i would require a lot of memory
  if(i %% 100 == 0){
    print(paste0(i,", ",Sys.time()))
  }
  
# Here I do the actual permutations using rbinom and create a new tilde_T variable. In each iteration, rbinom() runs again and creates a whole new vector of zeros and ones that will be stored in tilde_T, which is the column that takes the values of the treatment T_i. Then I create experiment_treatment_T. Also, note that the same person i must have the same value of tilde_T for every value of year_week, that is, if the individual is treated in the first week, they she must be treated in all weeks, and vice-versa. To do this, I first group the dataframe by personid and then use n=1 in the rbinom
performance_ri <- performance_ri %>%
  dplyr::group_by(personid) %>% # Groups the dataframe since we have clusters
  dplyr::mutate(
    tilde_T = rbinom(n = 1, size = 1, prob = 131/249), # tilde_T receives a new vector of zeros and ones. n = 1 since the dataframe is grouped at the individual level, size = 1 because we want a bernoulli trial (0 or 1) and prob is the parameter of the binomial
    experiment_treatment_T = experiment*tilde_T # Creates the experiment_treatment_T variable
  ) %>%
  dplyr::ungroup() # Ungroups the dataframe

# Now I estimate the regression.
reg1_ri <- performance_ri %>%
  estimatr::lm_robust(formula = perform1 ~ experiment_treatment_T,
                      clusters = personid,
                      fixed_effects = ~personid + year_week,
                      se_type = "stata")

# And save each t-statistic. I will simply take the t-value instead of calculating the statistic by hand to make things a bit faster. In each iteration i, I will store the t-statistic and the estimated beta in the i-th position of their respective vectors
tstat_ri[i] <- reg1_ri$statistic[1]
beta_ri[i] <- reg1_ri$coefficients[1]
  
# Loop ends here}

# And we are done!
```


```{r include=FALSE}
load("tstat_ri_10.Rdata")
load("beta_ri_10.Rdata")
```


With all $t(\tilde{T},Y)$ stored in ``tstat_ri`` and ``beta_ri``, we can calculate the proportion of these statistics that are larger than ``r reg1_tstat`` and ``r reg1_estimator``, respectively (remember we need to take absolute values, that is, we use ``abs()``):

```{r}
sum(abs(tstat_ri) > abs(reg1_tstat))/10^4
sum(abs(beta_ri) > abs(reg1_estimator))/10^4
```

The p-values obtained through permutation tests of the t-statistic $t = \frac{\hat{\beta}}{se(\hat{\beta})}$ and the estimator $\hat{\beta}$ are identical and equal to ``r format(sum(abs(tstat_ri) > abs(reg1_tstat))/10^4, scientific = F)``. These values are negligibly smaller than the p-value based on the asymptotic distribution of the estimator, ``r format(reg1$p.value[1], scientific = F)``.

The graphs below plot the density of the values obtained through the permutation tests and the value observed in the actual sample.

```{r}

ggplot() + 
  geom_density(mapping = aes(x = tstat_ri), fill = "grey") + 
  geom_vline(xintercept = reg1$statistic[1], color = "red") +
  geom_text(aes(x = reg1$statistic[1]+0.2), label = "Observed value", y = 0.05, angle = 90, vjust = -0.2) +
  labs(title = "Density of simulated t-statistics and observed value",x = "Values", y = "Density") +
  theme_bw(base_size = 10)

ggplot() + 
  geom_density(mapping = aes(x = beta_ri), fill = "grey") + 
  geom_vline(aes(xintercept = reg1$coefficients[1]), color = "red") +
  geom_text(aes(x = reg1$coefficients[1]+0.01), label = "Observed value", y = 1, angle = 90, vjust = -0.2) +
  labs(title = "Density of simulated estimators and observed value", x = "Values", y = "Density") +
  theme_bw(base_size = 10)
```


```{r include=FALSE}
load("tstat_ri.Rdata")
```


**Remark 2:** I also executed a for loop with ``P = 10^5`` permutations (the code took 4 hours to run), but only for the t-statistic (I got the question wrong, which I only realized after the aforementioned 4 hours). The calculated p-value was ``r format(sum(abs(tstat_ri) > abs(reg1_tstat))/10^5, scientific = F)``. Note that the p-value base on the asymptotic distribution is ``r format(reg1$p.value[1], scientific = F)``.

Because randomization inference relies on the exact distribution of the test statistic -- or close to it, since we are not performing all possible permutations -- under a sharp null, bad approximations or different null hypothesis cannot be a reason why the randomization inference p-values calculated using $\hat{\beta}$ and $t = \frac{\hat{\beta}}{se(\hat{\beta})}$ might be different. Instead, this difference comes from the fact that, being different statistics, they have different power against the alternative hypothesis of non-null intention to treat effect. Having different power, one could expect that they will result in different p-values. Still, both the estimator and the t-statistic have good power against such alternative hypothesis, which explain why RI p-values using each of them are identical. Small differences in these p-values are likely to be consequence of the simulation itself.

The RI p-values can differ from the p-value calculated using the asymptotic distribution of the estimator mostly because of bad or imprecise asymptotic approximations. Also, because the sharp null and the null in the asymptotic inference are not the same, p-values could also be different. It could be the case, for example, that the sharp null is false -- potential outcomes are different for at least one individual -- while the intention to treat effect is still zero.  

# Discuss potential problems in the experiment.  Discuss how you would be able to check whether these problems are relevant (for example, by looking at the available data or by collecting more data), or how the experiment could have been done differently to avoid such problems. Be clear about whether the potential problems you raised were discussed in the paper.

## Selection bias

As mentioned in question 1, the workers were previously surveyed by the firm, which assessed their qualifications and interest in joining the programs. One could argue that, in spirit, the research design had an oversubscription method. The firm was not interested in experimenting with the whole staff, so there was limited implementation capacity, and the demand for the program exceeded the supply. This clearly induced a selection bias in the experiment, since the design of the experiment was selected from a sample that was already interested in the program to begin with and which met the requirements.

Indeed, the authors report that those who volunteered for the program tended to have longer commuting time, less tenure in the firm, and be less educated. By meeting the requirements, they also had a tenure of at least six months, broadband access to the Internet, and an independent workspace. This is a potential problem because it may raise concerns about external validity. Also, the average treatment effect for the treatment is different than the average treatment effect, since those workers who volunteered for the program are probably those who can benefit more from it.

One could check if this problem is relevant by comparing the workers who participated to the rest of the population. The problem here is which population we are interested in. Arguably, if the population of interest is workers in general, the fact that the experiment was conducted on telemarketing workers already implies problems of external validity. As the authors themselves mention, the job of a call center is particularly suitable for WFH, as is IT support, secretarial assistance, etc. This is not true for every kind of worker, and WFH may be especially unsuitable for certain workers, as the pandemic has shown. Still, problems of external validity may arise in any research design, particularly one that investigated the effects of WFH. The authors recognize all of these issues.

It's hard to tell how the experiment could have been done differently to avoid this particular problem. The experiment was only possible to begin with because one of the authors, James Liang, is the former CEO of the company, a former Standford student, and a professor of Applied Economics. The firm was eager to experiment with WFH but concerned with its impacts. If the experiment had to be performed differently, it could not have been performed at all.

## Imperfect compliance

The design counted with imperfect compliance. The firm's policy obliged the workers who receive a certain treatment status to continue in their designed location of work for the whole duration of the experiment, but some workers who were working from home managed to go back to the office. In Figure V, it's possible to see that the share of employees working from home was not constant during the experiment, but fluctuated slightly around 80% to 90%.

The authors then perform the estimation using the birthdate status as treatment status which allows them to estimate the intention-to-treat (ITT) effect, which can be interpreted as the causal effect of being offered treatment. However, given the design of the experiment, I believe it would be more interesting to estimate the LATE. After all, the objective is to estimate how the change in the location of work, and working far from supervision, can affect performance. I am not sure if the authors didn't have access to the information of who defied treatment status.

To check if this problem is relevant, we could look at the proportion of workers who changed their location of work. The authors show this information in Figure V. Since the proportion of workers who changed is small and -- apparently -- every change was made from WFH to the office, and since the authors decide to estimate the ITT, I believe that it's unlikely that this problem is relevant.

Again, once the policy of the firm was not to allow these changes, but they still happened, then there isn't much else that could be done about the research design to avoid them.

## Hawthorne and Gift-Exchange Effects

The authors consider the possibility of Hawthorne and Gift-Exchange effects, which could bias the performance estimator and the satisfaction estimator upwards, respectively. They offer several arguments to convince us that the Hawthorne effect is not relevant. First: since each employee has a low impact on the overall evaluation, they would have little incentive to try to manipulate it. However, this assumes that the employees know that they are not pivotal, which is not always the case. If a large number of employees actually behave differently because they think it would make a difference, it could actually make a difference. Second: those employees who changed their minds and returned to the office performed no worse at the end of the treatment, in comparison to the beginning, suggesting that the reduced incentive to make the experiment succeed had no significant impact on their performance. However, if those employees ended up changing their minds, perhaps they were not so eager to work harder from home in the first place. The strongest evidence in the authors' favor is that, once the experiment ended, the gap in performance between office-based and home-based employees widened.

For the Gift-Exchange effect, the authors argue that the since "gift" (being allowed to work from home) was randomly allocated, workers wouldn't feel so grateful towards the firm in such a way as to change their behavior. Still, in the post-experiment survey, the question "How did working from home improve your performance?" received the fourth most common answer "Feeling more positive towards Ctrip for allowing me to work from home", with 12% of the responses. The authors then argue that Gift-Exchanged was only a secondary driver of the main effect. In this case, the post-experiment survey is a way of collecting data to check if the Gift-Exchange effect is relevant, but it has problems since it was collected after the treatment.

## Attrition

The authors document attrition rates of 17% in the treatment group and 35% in the control group. It's relevant to say that attrition itself is a parameter of interest since it was expected that WFH reduced attrition rates.

To test for selective attrition, the authors estimate probit models in which the dependent variable is whether the worker attrited or not. Not surprisingly, low-performance workers were more likely to quit their jobs in both the control and treatment groups. Also, by estimating the effect of performance on the probability of attrition in separate samples, the authors find that the low-performance workers who worked from home were much less likely to quit, even though they still did.

This would cause the performance estimator to be biased downwards. Since more low-performance workers in the control group leave in comparison to the treatment group, $Y_i(0)$ increases more than $Y_i(1)$ increases. Assuming monotonicity, the authors estimate Lee's bounds. The upper bound assumes that the workers with the lowest performance in the treatment group would quit in such a way as to create an equal attrition rate. The lower bound is analogous. The upper bound suggests that the effect is up to 50% larger than the estimated, but the lower bound is negative.
